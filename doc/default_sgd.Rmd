---
title: "Project4_Group1_Spring2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

In this project, we are going to ultilize matrix factorization methods for recommender system, aiming to match consumers with most appropriate products. Matrix factorization methods represent both items and users with vectors of factors inferred from item rating patterns. High correspondence between item and user factors leads to a recommendation. 

Our group has been assigned with the following algorithm, regularization and postprocessing:

- factorization algorithm: Stochastic Gradient Descent

- regularization: Penalty of Magnitudes + Bias and Interecepts vs Penalty of Magnitudes + Temporal Dynamics

- postpocessing: KNN

We are going to compare the regularization methods and evaluate their results.


### Step 1 Load Data and Train-test Split
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
data <- read.csv("../data/ml-latest-small/ratings.csv")

# did transformation on data for getting information in temporal dynamic
mean_time <- data%>%group_by(userId)%>%summarize(mean_time = mean(timestamp))
newdata <- left_join(data, mean_time, by="userId")
data <- newdata%>%mutate(sign = sign(timestamp-mean_time),diff_time = (abs(timestamp-mean_time)))

set.seed(0)
test_idx <- sample(1:nrow(data), round(nrow(data)/5, 0))
train_idx <- setdiff(1:nrow(data), test_idx)
data_train <- data[train_idx,]
data_test <- data[test_idx,]
```

###Step 2 Matrix Factorization

#### Step 2.1 Algorithm and Regularization
We perform stochastic gradien descent to do matrix factorization.

```{r}
U <- length(unique(data$userId))
I <- length(unique(data$movieId))
source("../lib/Matrix_Factorization.R")
```


#### Step 2.2 Parameter Tuning
Here we use cross validation to tune parameters: f as the dimension of factor and the penalty parameter $\lambda$ and select the best combination with the lowest test RMSE.

####Step 2.2.1 Parameter Tuning with only A1:Stochastic Gradient Descent 

```{r}
source("../lib/cross_validation.R")
f_list <- seq(10, 20, 10)
l_list <- seq(-2, -1, 1)
f_l <- expand.grid(f_list, l_list)
```


```{r, eval=FALSE}
# result_summary <- array(NA, dim = c(nrow(f_l), 10, 4)) 
# run_time <- system.time(for(i in 1:nrow(f_l)){
#     par <- paste("f = ", f_l[i,1], ", lambda = ", 10^f_l[i,2])
#     cat(par, "\n")
#     current_result <- cv.function(data, K = 5, f = f_l[i,1], lambda = 10^f_l[i,2])
#     result_summary[,,i] <- matrix(unlist(current_result), ncol = 10, byrow = T) 
#     print(result_summary)
#   
# })
# 
# save(result_summary, file = "../output/rmse.Rdata")
```


Plot the tuned parameters for A1: Stochastic Gradient Descent

```{r}
load("../output/rmse.Rdata")
rmse <- data.frame(rbind(t(result_summary[1,,]), t(result_summary[2,,])), train_test = rep(c("Train", "Test"), each = 4), par = rep(paste("f = ", f_l[,1], ", lambda = ", 10^f_l[,2]), times = 2)) %>% gather("epoch", "RMSE", -train_test, -par)
rmse$epoch <- as.numeric(gsub("X", "", rmse$epoch))
rmse %>% ggplot(aes(x = epoch, y = RMSE, col = train_test)) + geom_point() + facet_grid(~par)
```

From the graphs, we can observe that the f = 10. lambda = 0.1 maintain the best performance matrics with lowest test RMSE.


#### WX's part
#### Step 2.2.2 Parameter Tuning with R1 + R2

```{r}
source("../lib/cross_validation_r1+r2.R")
source("../lib/Matrix_Factorization_r1+r2.R")
source("../lib/Matrix_Factorization_r1r3.R")
f_list <- seq(10, 20, 10)
l_list <- seq(-2, -1, 1)
f_l <- expand.grid(f_list, l_list)

# result_summary_r12 <- array(NA, dim = c(nrow(f_l), 10, 4)) 
# run_time <- system.time(for(i in 1:nrow(f_l)){
#     par <- paste("f = ", f_l[i,1], ", lambda = ", 10^f_l[i,2])
#     cat(par, "\n")
#     current_result <- cv.function.r12(data, K = 5, f = f_l[i,1], lambda = 10^f_l[i,2])
#     result_summary_r12[,,i] <- matrix(unlist(current_result), ncol = 10, byrow = T) 
#     print(result_summary_r12)
#   
# })
# 
# save(result_summary_r12, file = "../output/rmseR12.Rdata")
```

####Plot the tunned parameters for R1 + R2

```{r}
load(file = "../output/rmseR12.Rdata")
rmse <- data.frame(rbind(t(result_summary_r12[1,,]), t(result_summary_r12[2,,])), train_test = rep(c("Train", "Test"), each = 4), par = rep(paste("f = ", f_l[,1], ", lambda = ", 10^f_l[,2]), times = 2)) %>% gather("epoch", "RMSE", -train_test, -par)
rmse$epoch <- as.numeric(gsub("X", "", rmse$epoch))
rmse %>% ggplot(aes(x = epoch, y = RMSE, col = train_test)) + geom_point() + facet_grid(~par)
```

From the graphs, we can observe that the f = ??. lambda = ?? maintain the best performance matrics with lowest test RMSE.

#### Step 3.1.1 Evaluation RMSE for A1 

```{r, warning=FALSE}
result <- gradesc(f = 10, lambda = 0.1,lrate = 0.01, max.iter = 100, stopping.deriv = 0.01,
                   data = data, train = data_train, test = data_test)

save(result, file = "../output/mat_fac.RData")
```


```{r}
#  load(file = "../output/rmse.Rdata")
#  load(file = "../output/mat_fac.Rdata")
#  
#  pred_rating <- t(result$q) %*% result$p
#  rmse_sgd <- RMSE2(data_test,pred_rating)
#  cat("The RMSE of A1 model is", rmse_sgd)
```


Here is the slelected parameter for A1+P2

```{r}
load(file = "../output/rmse.Rdata")
load(file = "../output/mat_fac.Rdata")
q <- result$q
p2_result_test <- pred_knn(data_train = data_train, data_test = data_test, q)
test_rmse_p2 <- p2_result_test['rmse_test']
cat("The RMSE of A1 with P2 model is", as.numeric(test_rmse_p2))
```

#### WX's part
#### Step 3.1.2 RMSE for R1+R2

```{r}
# resultR12 <- gradesc.r12(f = 10, lambda = 0.1,lrate = 0.01, max.iter = 100, stopping.deriv = 0.01,
#                    data = data, train = data_train, test = data_test)
# 
# save(resultR12, file = "../output/mat_fac_r12.RData")
```


Here is the selected set of hyperparameters for 
```{r, warning=FALSE}
load(file = "../output/mat_fac_r12.RData")
pred_rating <- t(resultr12$q) %*% resultr12$p
rmse_r12 <- RMSE2(data_test,pred_rating)
cat("The RMSE of A1 with R1 R2 model is", rmse_r12)
```

#### Step 3.1.3 RMSE for A1 + R1R3 + P2

```{r}
# result_R1R3 <- gradesc.r3(f = 10, lambda = 0.1,lrate = 0.01, max.iter = 100, stopping.deriv = 0.01,
#                    data = data[1:5000,], train = sub_data_train, test = sub_data_test)
# 
# save(result_R1R3, file = "../output/result_R1R3.RData")
```

```{r, warning=FALSE}
load(file = "../output/result_R1R3.RData")

pred_rating <- t(result_R1R3$q) %*% result_R1R3$p
rmse_r3 <- RMSE2(sub_data_test,pred_rating)
cat("The RMSE of A1 and R3 model is", rmse_r3)
```



### Step 3 Postprocessing with KNN
After matrix factorization, postporcessing will be performed to improve accuracy.

RMSE Function
```{r}
RMSE2 <- function(rating,est_rating){
  sqrt(mean((rating$rating-est_rating)^2))
}
```

KNN Function

```{r}
vec <- function(x) {
  
  return(sqrt(sum(x^2)))
  
  }

pred_knn <- function(data_train, data_test, q)
{
  
  norm_q <- apply(q,2,vec)
  sim <- t(t((t(q) %*% q)/ norm_q) / norm_q)
  colnames(sim) <- colnames(q)
  rownames(sim) <- colnames(q)
  pred_test <- rep(0,nrow(data_test))
  
  for (i in 1:nrow(data_test)){
    user_id <- data_test$userId[i]
    movie_id <- data_test$movieId[i]
    train <- data_train[data_train$userId == user_id & data_train$movieId != movie_id,]
    movie_train <- train$movieId
    sim_vec <- sim[rownames(sim) == movie_id, colnames(sim) %in% movie_train]
    movie <- names(sim_vec)[which.max(sim_vec)]
    pred_test[i] <- train[train$movieId == movie,][3]
  }
  
  pred_test <- as.matrix(unlist(pred_test))
  rmse_test <- sqrt(mean((data_test$rating-pred_test)^2))
  return(list(pred_test = pred_test, rmse_test = rmse_test))
  
}
```

###Step 3.3.2 RMSE for A1+R1+R3+P2

###RMSE for A1+P2
```{r}
q <- result$q
p2_result_test <- pred_knn(data_train = data_train, data_test = data_test, q)
test_rmse_p2 <- p2_result_test['rmse_test']
cat("The RMSE of A1 with P2 model is", as.numeric(test_rmse_p2))
```


###RMSE for A1+R1+R3+P2
```{r}
q <- result_R1R3$q
p2_result_test <- pred_knn(data_train = data_train, data_test = data_test, q)
test_rmse_p2 <- p2_result_test['rmse_test']
cat("The RMSE of A1 and R1+R3 with P2 model is", as.numeric(test_rmse_p2))
```



####Linear Regression


####Linear Regression for A1+R1+R2+P2



###Linear Regression for A1+R1+R2+P2



###Step 4: evaluation

Here we visualize training and testing RMSE by different dimension of factors and epochs

```{r}
library(ggplot2)

load(file = "../output/mat_fac.RData")
library(ggplot2)

RMSE <- data.frame(epochs = seq(10, 100, 10), Training_MSE = result$train_RMSE, Test_MSE = result$test_RMSE) %>% gather(key = train_or_test, value = RMSE, -epochs)

RMSE %>% ggplot(aes(x = epochs, y = RMSE,col = train_or_test)) + geom_point() + scale_x_discrete(limits = seq(10, 100, 10)) + xlim(c(0, 100))

```


####Result Summary

Train RMSE for A1 + P2:
Test RMSE for A1 + P2 : 1.16406

Train RMSE for A1 + R1R2 + P2 :
Test RMSE for A1 + R1R2 + P2 : 

Train RMSE for A1 + R1R3 + P2 :
Test RMSE for A1 + R1R3 + P2 : 1.317491

Linear regression results:

Train RMSE:
Test RMSE:

And the best parameters for all models is F = 10, lambda = 0.1 .
After comparing all the results, we find  has the best performance.

###Step 5 Further Discussion
####In order to maintain the 

